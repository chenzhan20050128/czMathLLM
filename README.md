HF_ENDPOINT=https://hf-mirror.com HF_HUB_ENABLE_HF_TRANSFER=1 python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='Qwen/Qwen3-4B-Thinking-2507',
    local_dir='/root/autodl-tmp/.autodl/czMathLLM/models/Qwen3-4B-Thinking-2507',
    resume_download=True,
    local_dir_use_symlinks=False,
    max_workers=8,  # å¯ç”¨å¤šçº¿ç¨‹ï¼ˆå»ºè®®4-8ä¸ªçº¿ç¨‹ï¼‰
)
"


# czMathLLM

czMathLLM æ˜¯ä¸€ä¸ªå›´ç»• **Unsloth + TRL** æ‰“é€ çš„æ•°å­¦æ•™å­¦/è§£é¢˜å¾®è°ƒå·¥å…·é›†ï¼Œé»˜è®¤é¢å‘ Qwen3 ç³»åˆ—æ¨¡å‹ã€‚é¡¹ç›®è¦†ç›–æ•°æ®å‡†å¤‡ã€ç›‘ç£å¼ LoRA/QLoRA å¾®è°ƒã€GRPO å¼ºåŒ–é˜¶æ®µã€ç¦»çº¿è¯„ä¼°ä¸æ¨ç†ä¸Šæœºçš„å®Œæ•´é—­ç¯ï¼Œå¹¶é€šè¿‡ `cli.py` æä¾›ä¸€ç«™å¼å‘½ä»¤è¡Œä½“éªŒã€‚

## é¡¹ç›®æ¦‚è§ˆ

- **æ¨¡å‹ç®¡ç† (`assets.py`)**ï¼šè‡ªåŠ¨ä¸‹è½½æˆ–å¤ç”¨åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒé€šè¿‡ `MATH_LLM_MODELS` é‡å®šå‘ç¼“å­˜ç›®å½•ã€‚
- **é…ç½®ä½“ç³» (`config.py`)**ï¼š`ProjectConfig` èšåˆè®­ç»ƒã€GRPOã€è¯„ä¼°ä¸‰å¤§é…ç½®ï¼›`DatasetSource` æ”¯æŒ HF ä»“åº“æˆ–æœ¬åœ° JSON/JSONLã€‚
- **æ•°æ®æµæ°´çº¿ (`data.py`)**ï¼šç»Ÿä¸€æŠ½å–é¢˜ç›®/ç­”æ¡ˆ/æ¨ç†é“¾ã€æ¨æ–­æœ€ç»ˆç­”æ¡ˆã€ç”Ÿæˆéš¾åº¦ä¸æ ‡ç­¾ï¼Œæ„å»º SFT ä¸ RL æ•°æ®é›†ã€‚
- **è®­ç»ƒæ‰§è¡Œ (`trainers/`)**ï¼š`run_sft_training` è´Ÿè´£ç›‘ç£å¾®è°ƒï¼Œ`run_grpo_training` æ‰§è¡Œå¼ºåŒ–å­¦ä¹ å¹¶å°è£…å¥–åŠ±å‡½æ•°ã€‚
- **æ¨ç†ä¸è¯„ä¼° (`modeling.py`ã€`evaluation.py`)**ï¼šå°è£…æ¨¡å‹åŠ è½½ã€åˆå¹¶ LoRAã€æ‰¹é‡ç”Ÿæˆç­”æ¡ˆä¸å¥–åŠ±è®¡ç®—ã€‚
- **å¥–åŠ±å‡½æ•° (`reward.py`)**ï¼šç»“åˆ \boxed{} è§£æã€æ•°å€¼æ¥è¿‘åº¦ä¸å­—ç¬¦ä¸²é‡åˆåº¦çš„ç»„åˆå¥–åŠ±ï¼Œé™„åŠ éš¾åº¦åŠ æƒã€‚

> å®Œæ•´ CLI å…¥å£ä½äº `cz_math_llm/cli_core.py`ï¼Œå¤–å±‚ `cli.py` ä»…åšå¯¼å…¥ä¸å¯åŠ¨ã€‚

## ç«¯åˆ°ç«¯æµç¨‹å¿«è§ˆ

1. **ç¯å¢ƒå‡†å¤‡**ï¼šå®‰è£…ä¸ GPU åŒ¹é…çš„ PyTorchã€Unslothã€TRL ç­‰ä¾èµ–ã€‚
2. **æ•°æ®é…ç½®**ï¼šç¼–å†™æˆ–å¤ç”¨ `DatasetSource` æè¿°ï¼ˆå¯ JSON å®šä¹‰ï¼‰ï¼Œè‡ªåŠ¨å®Œæˆå­—æ®µæ¸…æ´—ä¸å…ƒæ•°æ®æ„å»ºã€‚
3. **SFT è®­ç»ƒ**ï¼šæŒ‰æƒé‡æ··åˆæ¨ç†/æŒ‡ä»¤æ•°æ®ï¼Œåˆ›å»º LoRA é€‚é…å™¨å¹¶ä¿å­˜æ£€æŸ¥ç‚¹ä¸æ—¥å¿—ã€‚
4. **ï¼ˆå¯é€‰ï¼‰GRPO å¼ºåŒ–**ï¼šåŸºäºè‡ªå®šä¹‰æˆ–é»˜è®¤æ•°æ®é›†è¿›è¡Œå¥–åŠ±é©±åŠ¨è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡æ¨ç†è¡¨ç°ã€‚
5. **æ¨¡å‹åˆå¹¶**ï¼šé€‰æ‹©æ€§åœ°åˆå¹¶ LoRA æƒé‡ï¼Œå¾—åˆ°ä¾¿äºéƒ¨ç½²çš„å…¨é‡æ¨¡å‹ç›®å½•ã€‚
6. **ç¦»çº¿è¯„ä¼°**ï¼šæŒ‰ç›¸åŒé¢„å¤„ç†ç”Ÿæˆè¯„ä¼°é›†åˆï¼Œæ‰¹é‡ç®—åˆ†å¯¼å‡ºç»Ÿè®¡è¡¨ä¸ Parquetã€‚
7. **æ¨ç†ä¸Šçº¿**ï¼šä½¿ç”¨ `predict` å­å‘½ä»¤æˆ–ç›´æ¥è°ƒç”¨ `generate_answers` è¿›è¡Œé—®ç­”æµ‹è¯•æˆ–é›†æˆéƒ¨ç½²ã€‚

## ç¯å¢ƒä¸ç¡¬ä»¶è¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šLinux x86_64ï¼ˆå·²åœ¨ CUDA 12+ é©±åŠ¨ç¯å¢ƒéªŒè¯ï¼‰ã€‚
- **Python**ï¼šå»ºè®® 3.10â€“3.12ã€‚é¡¹ç›®é»˜è®¤ä½¿ç”¨ 3.12ã€‚
- **GPU**ï¼šæ˜¾å­˜ â‰¥ 16 GB å¯è¿è¡Œ Qwen3-4B LoRAï¼›32 GB ä»¥ä¸Šå¯é€‚åº¦æ”¾å®½æ‰¹é‡/åºåˆ—é•¿åº¦ã€‚
- **ä¾èµ–**ï¼š`torch 2.8.0`ï¼ˆCUDA 12.1+ï¼‰ã€`unsloth`, `trl`, `datasets`, `peft`, `bitsandbytes`, `accelerate` ç­‰ï¼Œè¯¦è§ `requirements.txt`ã€‚
- **Hugging Face**ï¼šè‹¥éœ€ä¸‹è½½å—é™æ¨¡å‹ï¼Œè¯· export `HF_TOKEN`ï¼›è®¾ç½® `HF_HUB_ENABLE_HF_TRANSFER=1` å¯æå‡ä¸‹è½½é€Ÿåº¦ã€‚

## å®‰è£…æ­¥éª¤

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
# è‹¥å®˜æ–¹æœªæä¾›åŒ¹é…çš„ CUDA è½®å­ï¼Œè¯·å…ˆå•ç‹¬å®‰è£… torch
pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```

å¯é€‰ï¼šæ‰§è¡Œ `huggingface-cli login` ä¿å­˜è®¿é—®ä»¤ç‰Œï¼›è®¾ç½® `export MATH_LLM_MODELS=/path/to/cache` æ§åˆ¶æ¨¡å‹ç¼“å­˜ä½ç½®ã€‚

> ğŸ” **ä¸è¦å°† Hugging Face ä»¤ç‰Œå†™å…¥ä»£ç æˆ–ä»“åº“ã€‚** æ¨èåœ¨ shell é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼š
> ```bash
export HF_TOKEN="---"
export MATH_LLM_PRIMARY_ENDPOINT="https://aliendao.cn"
export MATH_LLM_SECONDARY_ENDPOINT="https://hf-mirror.com"
> ```
> å¦‚æœªè®¾ç½®ï¼Œç¨‹åºä¼šé»˜è®¤æŒ‰ç…§ AlienDAO â†’ HF-Mirror â†’ å®˜æ–¹ç«™ çš„é¡ºåºå°è¯•ä¸‹è½½ã€‚

## æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼

é¡¹ç›®ä¼šåœ¨åŠ è½½é˜¶æ®µè‡ªåŠ¨å½’ä¸€åŒ–å­—æ®µï¼Œæ”¯æŒä¸‹åˆ—é”®åï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼š

| è¯­ä¹‰ | é»˜è®¤é”®å | å…¼å®¹é”® | è¯´æ˜ |
| --- | --- | --- | --- |
| é¢˜ç›® | `question` | `prompt`ã€`instruction`ã€`problem`ã€`input` | å¿…å¡« |
| æœ€ç»ˆç­”æ¡ˆ | `final_answer` | è‡ªåŠ¨æ¨æ–­ï¼Œæˆ– `final`, `answer_box` ç­‰ | å¯é€‰ |
| å‚è€ƒç­”æ¡ˆ | `answer` | `response`ã€`completion`ã€`target` ç­‰ | å¿…å¡« |
| æ¨ç†é“¾ | `reasoning` | `rationale`ã€`chain_of_thought`ã€`cot` ç­‰ | æ— åˆ™å›é€€åˆ° `answer` |

æœ¬åœ° JSON/JSONL ä¼šè¯»å–å…¨éƒ¨å¯¹è±¡å¹¶è°ƒç”¨ `_normalize_record` ç”Ÿæˆæ ‡å‡†å­—æ®µï¼Œå†è¿›ä¸€æ­¥æ¨æ–­ï¼š

- è‹¥ç¼ºå¤± `final_answer`ï¼Œå°è¯•è§£æ `\boxed{...}`ã€`Answer:`ã€ä¸­æ–‡â€œæœ€ç»ˆç­”æ¡ˆâ€ç­‰æ¨¡å¼ï¼›å¦åˆ™å–ç­”æ¡ˆæœ«è¡Œã€‚
- é¢å¤–ç”Ÿæˆ `metadata`ï¼ŒåŒ…å«é¢˜ç›®/æ¨ç†é•¿åº¦ç»Ÿè®¡ã€éš¾åº¦æ ‡ç­¾ (`easy/medium/hard`)ã€ä¸»é¢˜æ ‡ç­¾ï¼ˆå‡ ä½•/ä»£æ•°ç­‰ï¼‰ã€‚

### æ•°æ®é›†é…ç½®

- **å¿«é€Ÿä½“éªŒ**ï¼š`configs/dataset.sample.json` ç›´æ¥å¼•ç”¨æœ¬åœ°æ•°æ®ç›®å½•ï¼Œé»˜è®¤ 75% æ¨ç†å‹ï¼ˆ`data/OpenMathReasoning/data`ï¼‰+ 25% æŒ‡ä»¤å‹ï¼ˆ`data/DAPO-Math-17k-Processed/all`ï¼‰ã€‚
- **è‡ªå®šä¹‰æ··åˆ**ï¼šJSON æ–‡ä»¶åº”åŒ…å« `DatasetSource` åˆ—è¡¨ï¼Œæ¯é¡¹æ”¯æŒå­—æ®µï¼š`name`ã€`subset`ã€`split`ã€`path`ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰ã€`weight`ã€`reasoning`ã€`max_samples`ã€‚
- **å‘½ä»¤è¡Œè¦†ç›–**ï¼š`train/grpo/evaluate` å­å‘½ä»¤å‡æ”¯æŒ `--dataset-config` æˆ– `--reasoning-source` / `--instruction-source` / `--grpo-dataset`ã€‚å½“åŒæ—¶ä¼ å…¥æ—¶ï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›–é»˜è®¤é…ç½®ã€‚

## è®­ç»ƒæµç¨‹

```bash
python cli.py <command> [options]
```

### ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰

ç¤ºä¾‹å‘½ä»¤å‡è®¾ä½ å·²åœ¨ä»“åº“æ ¹ç›®å½•ä¸‹å‡†å¤‡å¥½ä»¥ä¸‹èµ„æºï¼š

- æ¨¡å‹ï¼š`models/Qwen3-4B-Thinking-2507`
- æ¨ç†æ•°æ®ï¼š`data/OpenMathReasoning/data`
- æŒ‡ä»¤æ•°æ®ï¼š`data/DAPO-Math-17k-Processed/all`

```bash
python cli.py train \
  --base-model-path models/Qwen3-4B-Thinking-2507 \
  --output-dir outputs/local_sft_big \
  --micro-batch-size 8 \
  --gradient-accumulation-steps 4 \
  --lora-rank 64 \
  --lora-dropout 0 \
  --learning-rate 2e-5 \
  --warmup-steps 50 \
  --max-steps 1600 \
  --logging-steps 10 \
  --eval-steps 25 \
  --save-steps 50 \
  --save-total-limit 3 \
  --dataset-num-proc 4 \
  --resume-from-checkpoint outputs/local_sft/checkpoints/checkpoint-200

```

æ‰§è¡Œé€»è¾‘ï¼ˆ`run_sft_training`ï¼‰ï¼š

1. `assets.ensure_model` ç¡®ä¿åŸºç¡€æ¨¡å‹å¯ç”¨ï¼ŒæŒ‰éœ€ä¸‹è½½ã€‚
2. `load_base_model` ä»¥ 4-bit é‡åŒ–åŠ è½½æ¨¡å‹ï¼›`prepare_lora_model` æ ¹æ®é…ç½®åˆ›å»º LoRA é€‚é…å™¨ï¼Œæ”¯æŒ RS-LoRAã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€AdamW 8bit ç­‰ã€‚
3. `build_sft_datasets` è¯»å–å¹¶æ‹¼æ¥å¤šæ•°æ®æºï¼Œä½¿ç”¨ `format_sft_example` ç”Ÿæˆç¬¦åˆ Qwen3 æ¨¡æ¿çš„å¯¹è¯æ–‡æœ¬ï¼Œå¹¶éšæœºåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ã€‚
4. `SFTTrainer` è´Ÿè´£è®­ç»ƒä¸è¯„ä¼°ï¼Œå‘¨æœŸæ€§å†™å…¥æ—¥å¿—ã€ä¿å­˜æ£€æŸ¥ç‚¹åˆ° `outputs/<experiment>_lora` ä¸ `outputs/checkpoints`ã€‚
5. è®­ç»ƒç»“æŸåä¿å­˜ LoRA æƒé‡ã€åˆ†è¯å™¨ï¼Œå¹¶å¯é€‰è°ƒç”¨ `merge_and_save` å°† LoRA åˆå¹¶ä¸ºå…¨é‡æ¨¡å‹ï¼ˆ`outputs/<experiment>_merged`ï¼‰ã€‚

å¸¸ç”¨å‚æ•°ï¼š`--micro-batch-size`ã€`--gradient-accumulation-steps`ã€`--learning-rate`ã€`--num-train-epochs`ã€`--load-in-4bit/--no-4bit`ã€`--full-finetuning`ã€‚

### GRPO å¼ºåŒ–é˜¶æ®µï¼ˆå¯é€‰ï¼‰

```bash
python cli.py train \
  --reasoning-source data/OpenMathReasoning/data \
  --instruction-source data/DAPO-Math-17k-Processed/all \
  --base-model-path models/Qwen3-4B-Thinking-2507 \
  --output-dir outputs/local_sft \
  --with-grpo --grpo-steps 300 \
  --grpo-learning-rate 5e-6 --grpo-beta 0.2
```

æˆ–å•ç‹¬è¿è¡Œï¼š

```bash
python cli.py grpo \
  --grpo-dataset data/OpenMathReasoning/data \
  --base-model-path models/Qwen3-4B-Thinking-2507 \
  --output-dir outputs/local_sft \
  --grpo-steps 400 \
  --resume-from-checkpoint outputs/local_sft/checkpoints/last
```

æ‰§è¡Œé€»è¾‘ï¼ˆ`run_grpo_training`ï¼‰ï¼š

1. è½½å…¥ä¸Šä¸€é˜¶æ®µ LoRA æƒé‡ï¼Œå¹¶æŒ‰éœ€åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆ`reference_free=True` æ—¶è·³è¿‡ï¼‰ã€‚
2. æ„å»º RL æ•°æ®é›†ï¼šè‹¥æœªæ˜¾å¼æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨ SFT è®­ç»ƒé›†ï¼›å¦åˆ™æŒ‰ `DatasetSource` é‡æ–°åŠ è½½å¹¶è½¬åŒ–ä¸º `prompt/reference/metadata` ç»“æ„ã€‚
3. å®šåˆ¶å¥–åŠ±ï¼š`reward_fn` è°ƒç”¨ `batch_reward` æ ¹æ®é¢„æµ‹ä¸å‚è€ƒç­”æ¡ˆçš„æ•°å€¼/æ–‡æœ¬æ¥è¿‘åº¦ç»™åˆ†ï¼Œå¹¶å¯¹é«˜éš¾åº¦é•¿æ¨ç†ç»™äºˆå¥–åŠ±å¢ç›Šã€‚
4. å°†é¡¹ç›®é…ç½®è½¬è¯‘ä¸º `HFGRPOConfig`ï¼Œå…¼å®¹ä¸åŒ TRL ç‰ˆæœ¬çš„å‚æ•°ç­¾åã€‚
5. `GRPOTrainer` è¿è¡Œå¼ºåŒ–è®­ç»ƒï¼Œå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæœ€ç»ˆå†™å› LoRA ç›®å½•å¹¶å†æ¬¡å°è¯•æ¨¡å‹åˆå¹¶ã€‚

å…³é”®å‚æ•°ï¼š`--grpo-steps`ï¼ˆæˆ– `HFGRPOConfig.total_episodes`ï¼‰ã€`--grpo-mini-batch`ã€`--grpo-learning-rate`ã€`--grpo-beta`ã€`--grpo-kl`ã€`--grpo-reference-free`ã€‚

### å¸¸ç”¨ CLI å‚æ•°å¯¹ç…§

| å­å‘½ä»¤ | å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
| --- | --- | --- | --- |
| train/grpo | `--dataset-config` | æŒ‡å‘ JSON æ•°æ®é›†æ··åˆé…ç½® | ä¸ºç©ºåˆ™ä½¿ç”¨å†…ç½®é»˜è®¤ mix |
| train | `--max-seq-length` | æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ | 4096 |
| train | `--lora-rank / --lora-alpha / --lora-dropout` | LoRA è¶…å‚ | 64 / 64 / 0.05 |
| train | `--save-merged-model` | æ˜¯å¦ä¿å­˜åˆå¹¶æ¨¡å‹ï¼ˆfp16/bf16ï¼‰ | å¼€å¯ |
| grpo | `--grpo-dataset` | å•ç‹¬æŒ‡å®šå¼ºåŒ–æ•°æ®æº | é»˜è®¤ä¸º SFT è®­ç»ƒé›† |
| evaluate | `--sample-size` | è¯„ä¼°æ—¶é‡‡æ ·æ•°é‡ | `EvaluationConfig.sample_size` |
| predict | `--adapter-path` | æŒ‡å®šç‹¬ç«‹ LoRA ç›®å½• | ä¸ä¼ åˆ™ä½¿ç”¨åˆå¹¶/åŸå§‹æ¨¡å‹ |

## è¯„ä¼°ä¸åº¦é‡

```bash
python cli.py evaluate \
  --dataset-config configs/dataset.sample.json \
  --model-path outputs/sample_run_merged \
  --sample-size 100 \
  --save-path outputs/eval.parquet
```

è¯„ä¼°æµç¨‹ï¼š

1. ä½¿ç”¨ `build_sft_datasets` å¤ç°è®­ç»ƒé˜¶æ®µçš„æ¸…æ´—ä¸åˆ‡åˆ†ï¼Œå¹¶é‡‡æ ·å›ºå®šæ•°é‡é—®é¢˜ã€‚
2. `format_inference_prompt` ç”Ÿæˆç³»ç»Ÿæç¤º + ç”¨æˆ·é—®é¢˜æ ¼å¼ï¼Œè°ƒç”¨ `generate_answers` æ‰¹é‡æ¨ç†ã€‚
3. é€šè¿‡ `batch_reward` è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼Œå¹¶è¾“å‡º `question/reference/generation/reward` å­—æ®µçš„æ•°æ®æ¡†ï¼Œå¯ç›´æ¥ `describe()` è·å–ç»Ÿè®¡å€¼ã€‚
4. å¦‚æŒ‡å®š `--save-path`ï¼Œå°†ç»“æœä¿å­˜ä¸º Parquet ä¾›è¿›ä¸€æ­¥åˆ†æã€‚

## æ¨ç†ä¸æ¨¡å‹å‘å¸ƒ

```bash
python cli.py predict \
  --model-path outputs/sample_run_merged \
  --question "è¯·è¯æ˜å‹¾è‚¡å®šç†å¹¶ç»™å‡ºç¤ºä¾‹ã€‚" \
  --max-new-tokens 512
```

ä½¿ç”¨å•ç‹¬ LoRA é€‚é…å™¨ï¼š

```bash
python cli.py predict \
  --base-model-id unsloth/Qwen3-4B-Instruct \
  --adapter-path outputs/sample_run_lora \
  --question "è®¡ç®— \int_0^1 x^2 dx å¹¶è§£é‡Šæ­¥éª¤"
```

`prepare_for_inference` ä¼šåˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹© GPU/CPUï¼›`generate_answers` æ”¯æŒæ‰¹é‡è¾“å…¥å’Œå¯é…ç½®çš„ `max_new_tokens`ã€‚

## è¾“å‡ºç›®å½•çº¦å®š

| è·¯å¾„ | è¯´æ˜ |
| --- | --- |
| `outputs/<experiment>_lora` | LoRA æƒé‡ï¼ˆ`adapter_config.json`, `adapter_model.safetensors` ç­‰ï¼‰ä¸åˆ†è¯å™¨ |
| `outputs/<experiment>_merged` | åˆå¹¶åçš„å…¨é‡æƒé‡ï¼ˆè‹¥ `save_merged_model=True`ï¼‰ |
| `outputs/checkpoints` | é€æ­¥ä¿å­˜çš„è®­ç»ƒæ£€æŸ¥ç‚¹ã€æœ€æ–°çŠ¶æ€ `last` |
| `outputs/logs`ï¼ˆéœ€è‡ªè¡Œåˆ›å»ºï¼‰ | æ¨èå°† TensorBoard/è‡ªå®šä¹‰æ—¥å¿—å†™å…¥æ­¤å¤„ |

## é…ç½®å‚è€ƒ

### TrainingConfig å…³é”®å­—æ®µ

- `base_model_id` / `base_model_path`ï¼šHF æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„ï¼›è‹¥åŒæ—¶æä¾›ï¼Œä»¥ `base_model_path` ä¸ºå‡†ã€‚
- `load_in_4bit` / `load_in_8bit` / `full_finetuning`ï¼šæ§åˆ¶é‡åŒ–ä¸å¾®è°ƒæ¨¡å¼ã€‚
- `micro_batch_size`ã€`gradient_accumulation_steps`ã€`learning_rate`ã€`warmup_steps` ç­‰ï¼šSFT è®­ç»ƒæ ¸å¿ƒè¶…å‚ã€‚
- `dataset_mix`ï¼š`DatasetSource` åºåˆ—ï¼ŒåŒ…å«æƒé‡ä¸æ˜¯å¦æ¨ç†å‹æ ‡è®°ã€‚
- `save_merged_model`ã€`merge_dtype`ï¼šæ§åˆ¶æ˜¯å¦åˆå¹¶ LoRA ä»¥åŠè¾“å‡ºç²¾åº¦ï¼ˆ`fp16`/`bf16`ï¼‰ã€‚

### GRPOConfig ä¸ EvaluationConfig

- `GRPOConfig.steps`ï¼šRL æ€»æ­¥æ•°ï¼ŒåŒæ­¥å†³å®šæ•°æ®é›†æˆªæ–­é•¿åº¦ã€‚
- `reference_free`ï¼šå…³é—­å‚è€ƒæ¨¡å‹ï¼Œå¯åœ¨èµ„æºå—é™æ—¶å‡å°‘æ˜¾å­˜ã€‚
- `num_generations_per_prompt`ï¼šTR L ç‰ˆæœ¬æ”¯æŒæ—¶å¯é…ç½®æ¯æ¬¡ç”Ÿæˆæ•°é‡ã€‚
- `EvaluationConfig.system_prompt`ï¼šå¯è‡ªå®šä¹‰è¯„ä¼°æ—¶çš„ç³»ç»Ÿæç¤ºï¼Œä¿æŒä¸ç”Ÿäº§ä¸€è‡´ã€‚

## ç¯å¢ƒå˜é‡ä¸€è§ˆ

- `MATH_LLM_MODELS`ï¼šæ¨¡å‹ç¼“å­˜æ ¹ç›®å½•ï¼Œé»˜è®¤ `./models`ã€‚
- `MATH_LLM_PRIMARY_ENDPOINT`ï¼šé¦–é€‰é•œåƒç«¯ç‚¹ï¼Œé»˜è®¤ `https://aliendao.cn`ã€‚
- `MATH_LLM_SECONDARY_ENDPOINT`ï¼šç¬¬äºŒé•œåƒç«¯ç‚¹ï¼Œé»˜è®¤ `https://hf-mirror.com`ã€‚
- `HF_TOKEN`ï¼šè®¿é—®ç§æœ‰æˆ–å—é™æ¨¡å‹æ‰€éœ€ã€‚
- `HF_HUB_ENABLE_HF_TRANSFER`ï¼šè®¾ç½®ä¸º `1` æ—¶å¯ç”¨åŠ é€Ÿä¸‹è½½ã€‚
- `CUDA_VISIBLE_DEVICES`ï¼šæ§åˆ¶å¯è§ GPUï¼Œä¸ `--micro-batch-size` åè°ƒè®¾ç½®æ˜¾å­˜å ç”¨ã€‚

## è°ƒè¯•ä¸æœ€ä½³å®è·µ

- **æ˜¾å­˜æº¢å‡º**ï¼šé™ä½ `--micro-batch-size` æˆ– `--max-seq-length`ï¼Œå¿…è¦æ—¶å…³é—­ `--save-merged-model` å‡å°‘æ˜¾å­˜å³°å€¼ã€‚
- **æ•°æ®è´¨é‡**ï¼šæ£€æŸ¥ JSONL æ˜¯å¦åŒ…å«ç©ºè¡Œæˆ–æ— æ•ˆå­—æ®µï¼›`_normalize_record` åœ¨å­—æ®µç¼ºå¤±æ—¶ä¼šæŠ›å‡ºæŠ¥é”™ã€‚
- **éšæœºæ€§æ§åˆ¶**ï¼š`TrainingConfig.random_seed` ç»Ÿä¸€è®¾ç½® Python/NumPy/PyTorch éšæœºç§å­ï¼Œä¿è¯å¤ç°æ€§ã€‚
- **å¤šæœºä¸‹è½½**ï¼šæå‰è¿è¡Œ `python -c "from cz_math_llm.assets import ensure_model; ensure_model('unsloth/Qwen3-4B-Instruct')"` é¢„çƒ­ç¼“å­˜ã€‚
- **å¥–åŠ±è°ƒä¼˜**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹å®šåˆ¶ `reward.py`ï¼Œä¾‹å¦‚åŠ å…¥ç»´åº¦æ‰“åˆ†ã€æ ¼å¼æ ¡éªŒæˆ–å¼•ç”¨å¤–éƒ¨åˆ¤é¢˜å™¨ã€‚

## ç¤ºä¾‹æ•°æ®

- `data/OpenMathReasoning/data`ï¼šå®Œæ•´æ¨ç†æ•°æ®ï¼ˆParquetï¼‰ï¼Œä¸å‘½ä»¤è¡Œç¤ºä¾‹ä¸€è‡´ã€‚
- `data/DAPO-Math-17k-Processed/all`ï¼šæŒ‡ä»¤æ•°æ®ï¼ˆParquetï¼‰ï¼Œç”¨äºè¡¥å……ç›‘ç£ä¿¡å·ã€‚
- `data/sample_reasoning.jsonl`ï¼šå« `<think>...</think>` æ¨ç†é“¾çš„æ•°å­¦é¢˜ç¤ºä¾‹ã€‚
- `data/sample_instruction.jsonl`ï¼šç®€å•æŒ‡ä»¤ Q&Aï¼Œé€‚åˆæµ‹è¯•æ··åˆæ•°æ®ç®¡çº¿ã€‚
- `configs/dataset.sample.json`ï¼šæ•°æ®æ··åˆæ ·æ¿ï¼Œå¯å¤åˆ¶åæ›¿æ¢ `name/subset/split/weight`ã€‚

## ä¸‹ä¸€æ­¥

- æ›¿æ¢ç¤ºä¾‹æ•°æ®ä¸ºçœŸå®æ•°å­¦é¢˜åº“ï¼ˆå¦‚ AMC/AIMEã€ç«èµ›é¢˜ç­‰ï¼‰ï¼Œå¹¶æ‰©å……éš¾åº¦æ ‡ç­¾ã€‚
- é’ˆå¯¹ä¸åŒ GPU èµ„æºè°ƒèŠ‚ LoRA/GRPO è¶…å‚ï¼Œè®°å½•å®éªŒé…ç½®ï¼ˆå»ºè®®é…åˆ `utils.dump_dataclass` è¾“å‡ºé…ç½®å¿«ç…§ï¼‰ã€‚
- ç»“åˆè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼ˆBLEUã€ç¬¦å·å¯¹é½ç­‰ï¼‰æˆ–å¼•å…¥å¤–éƒ¨åˆ¤é¢˜å™¨ï¼Œå®Œå–„ `reward.py`ã€‚
- å°† CLI æµç¨‹å°è£…ä¸º CI/CD ä»»åŠ¡æˆ– Notebookï¼Œä¾¿äºååŒè°ƒè¯•ã€‚

ç¥è®­ç»ƒé¡ºåˆ©ï¼ŒEnjoy math teaching with Qwen3! ğŸ“



```text

(base) root@autodl-container-7702429a5b-ca7e9638:~/autodl-tmp/.autodl/czMathLLM# tree
.
â”œâ”€â”€ README.md
â”œâ”€â”€ cli.py
â”œâ”€â”€ configs
â”‚   â””â”€â”€ dataset.sample.json
â”œâ”€â”€ cz_math_llm
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ assets.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ cli_core.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ config.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ data.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ evaluation.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ modeling.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ prompts.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ reward.cpython-312.pyc
â”‚   â”‚   â””â”€â”€ utils.cpython-312.pyc
â”‚   â”œâ”€â”€ assets.py
â”‚   â”œâ”€â”€ cli_core.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ quick_check.log
â”‚   â”œâ”€â”€ reward.py
â”‚   â”œâ”€â”€ trainers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ grpo.cpython-312.pyc
â”‚   â”‚   â”‚   â””â”€â”€ sft.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ grpo.py
â”‚   â”‚   â””â”€â”€ sft.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ DAPO-Math-17k-Processed
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ all
â”‚   â”‚   â”‚   â””â”€â”€ train-00000-of-00001.parquet
â”‚   â”‚   â”œâ”€â”€ cn
â”‚   â”‚   â”‚   â””â”€â”€ train-00000-of-00001.parquet
â”‚   â”‚   â”œâ”€â”€ create_dataset.py
â”‚   â”‚   â””â”€â”€ en
â”‚   â”‚       â””â”€â”€ train-00000-of-00001.parquet
â”‚   â”œâ”€â”€ OpenMathReasoning
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ data
â”‚   â”‚       â””â”€â”€ cot-00000-of-00001.parquet
â”‚   â”œâ”€â”€ OpenMathReasoningFull
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”‚   â”œâ”€â”€ additional_problems-00000-of-00001.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00000-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00001-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00002-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00003-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00004-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00005-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00006-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00007-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00008-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00010-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00011-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00012-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00013-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00014-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00015-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00016-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00017-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00018-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00019-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00020-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00021-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00022-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00023-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00024-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00025-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00026-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00027-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00028-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00029-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00030-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00031-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00032-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00033-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00034-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00035-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00036-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00037-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00038-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00039-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00040-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00041-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00042-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00043-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00044-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00045-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00046-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00047-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00048-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00049-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00050-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00051-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00052-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00053-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00054-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00055-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00056-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00057-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00058-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00059-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00060-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00061-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00062-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00063-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00064-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00065-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00066-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00067-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00068-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00069-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00070-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00071-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00072-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00073-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00074-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00075-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00076-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00077-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00078-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00079-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00080-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00081-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00082-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00083-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00084-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00085-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00086-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00087-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00088-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00089-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00090-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00091-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00092-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00093-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00094-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00095-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00096-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00097-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00098-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00099-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00100-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00101-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00102-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00103-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00104-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00105-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00106-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00107-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00108-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00109-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00110-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00111-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00112-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00113-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00114-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00115-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00116-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00117-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00118-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00119-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00120-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00121-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00122-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00123-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00124-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00125-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00126-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00127-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00128-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00129-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00130-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00131-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00132-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00133-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00134-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00135-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00136-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00137-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00138-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00139-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00140-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00141-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00142-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ cot-00143-of-00144.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00000-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00001-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00002-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00003-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00004-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00005-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00006-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00007-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00008-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00009-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00010-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00011-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00012-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ genselect-00013-of-00014.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00000-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00001-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00002-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00003-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00004-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00005-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00006-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00007-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00008-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00009-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00010-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00011-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00012-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00013-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00014-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00015-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00016-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00017-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00018-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00019-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00020-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00021-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00022-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00023-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00024-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00025-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00026-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00027-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00028-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00029-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00030-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00031-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00032-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00033-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00034-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00035-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00036-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00037-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00038-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00039-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00040-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00041-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00042-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00043-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00044-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00045-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00046-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00047-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00048-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00049-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00050-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00051-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00052-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00053-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00054-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00055-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00056-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00057-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00058-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00059-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00060-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00061-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00062-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00063-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00064-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00065-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00066-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00067-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00068-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00069-of-00072.parquet
â”‚   â”‚   â”‚   â”œâ”€â”€ tir-00070-of-00072.parquet
â”‚   â”‚   â”‚   â””â”€â”€ tir-00071-of-00072.parquet
â”‚   â”‚   â”œâ”€â”€ download_dataset.py
â”‚   â”‚   â””â”€â”€ results.png
â”‚   â”œâ”€â”€ sample_instruction.jsonl
â”‚   â””â”€â”€ sample_reasoning.jsonl
â”œâ”€â”€ law-finetune-code.txt
â”œâ”€â”€ models
â”‚   â””â”€â”€ Qwen3-4B-Thinking-2507
â”‚       â”œâ”€â”€ LICENSE
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ merges.txt
â”‚       â”œâ”€â”€ model-00001-of-00003.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00003.safetensors
â”‚       â”œâ”€â”€ model-00003-of-00003.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â””â”€â”€ vocab.json
â”œâ”€â”€ outputs
â”‚   â”œâ”€â”€ local_sft
â”‚   â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoint-200
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ optimizer.pt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rng_state.pth
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler.pt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â”‚   â”‚   â”œâ”€â”€ last
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â”‚   â”‚   â””â”€â”€ trainer_state.json
â”‚   â”‚   â”œâ”€â”€ qwen_math_tutor_lora
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â”‚   â””â”€â”€ qwen_math_tutor_merged
â”‚   â”‚       â”œâ”€â”€ added_tokens.json
â”‚   â”‚       â”œâ”€â”€ chat_template.jinja
â”‚   â”‚       â”œâ”€â”€ merges.txt
â”‚   â”‚       â”œâ”€â”€ model-00001-of-00003.safetensors
â”‚   â”‚       â”œâ”€â”€ model-00002-of-00003.safetensors
â”‚   â”‚       â”œâ”€â”€ model-00003-of-00003.safetensors
â”‚   â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚   â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚       â”œâ”€â”€ tokenizer.json
â”‚   â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚       â””â”€â”€ vocab.json
â”‚   â”œâ”€â”€ local_sft_big
â”‚   â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”‚   â””â”€â”€ runs
â”‚   â”‚   â”‚       â”œâ”€â”€ Nov12_11-58-55_autodl-container-7702429a5b-ca7e9638
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762919936.autodl-container-7702429a5b-ca7e9638.26652.0
â”‚   â”‚   â”‚       â””â”€â”€ Nov12_12-13-56_autodl-container-7702429a5b-ca7e9638
â”‚   â”‚   â”‚           â””â”€â”€ events.out.tfevents.1762920839.autodl-container-7702429a5b-ca7e9638.4000.0
â”‚   â”‚   â”œâ”€â”€ qwen_math_tutor_lora
â”‚   â”‚   â””â”€â”€ qwen_math_tutor_merged
â”‚   â”œâ”€â”€ qwen3_4b_test
â”‚   â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”œâ”€â”€ qwen_math_tutor_lora
â”‚   â”‚   â””â”€â”€ qwen_math_tutor_merged
â”‚   â”œâ”€â”€ test_run
â”‚   â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”œâ”€â”€ qwen_math_tutor_lora
â”‚   â”‚   â””â”€â”€ qwen_math_tutor_merged
â”‚   â””â”€â”€ token_test
â”‚       â”œâ”€â”€ checkpoints
â”‚       â”œâ”€â”€ qwen_math_tutor_lora
â”‚       â””â”€â”€ qwen_math_tutor_merged
â”œâ”€â”€ quick_check.log
â”œâ”€â”€ qwenFineTuning.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tmp_qwen_test
â”‚   â””â”€â”€ config.json
â”œâ”€â”€ trainer_output
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ checkpoint-16
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”œâ”€â”€ optimizer.pt
â”‚   â”‚   â”œâ”€â”€ rng_state.pth
â”‚   â”‚   â”œâ”€â”€ scheduler.pt
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â”œâ”€â”€ checkpoint-4
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”œâ”€â”€ optimizer.pt
â”‚   â”‚   â”œâ”€â”€ rng_state.pth
â”‚   â”‚   â”œâ”€â”€ scheduler.pt
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â”œâ”€â”€ checkpoint-8
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”‚   â”œâ”€â”€ chat_template.jinja
â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”œâ”€â”€ optimizer.pt
â”‚   â”‚   â”œâ”€â”€ rng_state.pth
â”‚   â”‚   â”œâ”€â”€ scheduler.pt
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â””â”€â”€ runs
â”‚       â”œâ”€â”€ Nov11_21-40-47_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762868448.autodl-container-7702429a5b-ca7e9638.32383.0
â”‚       â”œâ”€â”€ Nov11_21-42-43_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762868564.autodl-container-7702429a5b-ca7e9638.32864.0
â”‚       â”œâ”€â”€ Nov11_21-45-16_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762868717.autodl-container-7702429a5b-ca7e9638.33265.0
â”‚       â”œâ”€â”€ Nov11_21-48-03_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762868884.autodl-container-7702429a5b-ca7e9638.34023.0
â”‚       â”œâ”€â”€ Nov12_10-18-31_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762913912.autodl-container-7702429a5b-ca7e9638.7912.0
â”‚       â”œâ”€â”€ Nov12_10-23-28_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762914209.autodl-container-7702429a5b-ca7e9638.10371.0
â”‚       â”œâ”€â”€ Nov12_10-28-24_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762914505.autodl-container-7702429a5b-ca7e9638.14042.0
â”‚       â”œâ”€â”€ Nov12_10-33-23_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762914804.autodl-container-7702429a5b-ca7e9638.14796.0
â”‚       â”œâ”€â”€ Nov12_10-42-00_autodl-container-7702429a5b-ca7e9638
â”‚       â”‚   â””â”€â”€ events.out.tfevents.1762915322.autodl-container-7702429a5b-ca7e9638.15719.0
â”‚       â””â”€â”€ Nov12_10-56-04_autodl-container-7702429a5b-ca7e9638
â”‚           â””â”€â”€ events.out.tfevents.1762916166.autodl-container-7702429a5b-ca7e9638.17027.0
â”œâ”€â”€ unsloth_compiled_cache
â”‚   â”œâ”€â”€ UnslothAlignPropTrainer.py
â”‚   â”œâ”€â”€ UnslothBCOTrainer.py
â”‚   â”œâ”€â”€ UnslothCPOTrainer.py
â”‚   â”œâ”€â”€ UnslothDDPOTrainer.py
â”‚   â”œâ”€â”€ UnslothDPOTrainer.py
â”‚   â”œâ”€â”€ UnslothGKDTrainer.py
â”‚   â”œâ”€â”€ UnslothGRPOTrainer.py
â”‚   â”œâ”€â”€ UnslothIterativeSFTTrainer.py
â”‚   â”œâ”€â”€ UnslothKTOTrainer.py
â”‚   â”œâ”€â”€ UnslothNashMDTrainer.py
â”‚   â”œâ”€â”€ UnslothORPOTrainer.py
â”‚   â”œâ”€â”€ UnslothOnlineDPOTrainer.py
â”‚   â”œâ”€â”€ UnslothPPOTrainer.py
â”‚   â”œâ”€â”€ UnslothPRMTrainer.py
â”‚   â”œâ”€â”€ UnslothRLOOTrainer.py
â”‚   â”œâ”€â”€ UnslothRewardTrainer.py
â”‚   â”œâ”€â”€ UnslothSFTTrainer.py
â”‚   â”œâ”€â”€ UnslothXPOTrainer.py
â”‚   â””â”€â”€ __pycache__
â”‚       â”œâ”€â”€ UnslothAlignPropTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothBCOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothCPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothDDPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothDPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothGKDTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothGRPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothIterativeSFTTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothKTOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothNashMDTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothORPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothOnlineDPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothPPOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothPRMTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothRLOOTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothRewardTrainer.cpython-312.pyc
â”‚       â”œâ”€â”€ UnslothSFTTrainer.cpython-312.pyc
â”‚       â””â”€â”€ UnslothXPOTrainer.cpython-312.pyc
â””â”€â”€ unsloth_training_checkpoints

61 directories, 433 files

```
